{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb0f58f-463f-4a49-891c-e7a9a9b63b35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (2.0.22)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (3.0.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.11/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer in /opt/conda/lib/python3.11/site-packages (from kaggle) (3.3.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from kaggle) (3.4)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.8.2)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from kaggle) (68.2.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.66.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.0.7)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from kaggle) (0.5.1)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: text-unidecode, python-slugify, python-dotenv, psycopg2-binary, kaggle\n",
      "Successfully installed kaggle-1.7.4.2 psycopg2-binary-2.9.10 python-dotenv-1.1.0 python-slugify-8.0.4 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy psycopg2-binary python-dotenv kaggle pytest tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdbaf5b9-4a2e-493d-ad46-a45804d2ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bắt đầu thiết lập Kaggle Credentials ---\n",
      "2025-04-07 18:03:17,053 - INFO - Tìm thấy file nguồn: /home/jovyan/work/kaggle.json\n",
      "2025-04-07 18:03:17,054 - INFO - Đã tạo (hoặc tồn tại) thư mục: /home/jovyan/.config/kaggle\n",
      "2025-04-07 18:03:17,054 - INFO - Đã copy file vào: /home/jovyan/.config/kaggle/kaggle.json\n",
      "2025-04-07 18:03:17,054 - INFO - Đã thiết lập quyền truy cập (600) cho: /home/jovyan/.config/kaggle/kaggle.json\n",
      "\n",
      "--- Thông tin Credentials (đọc từ file đã copy) ---\n",
      "Kaggle Username: thangalbert\n",
      "Đã đọc thành công key.\n",
      "----------------------------------------\n",
      "2025-04-07 18:03:17,055 - INFO - Credentials đã được thiết lập tại ~/.config/kaggle/kaggle.json.\n",
      "2025-04-07 18:03:17,055 - INFO - Bây giờ bạn có thể sử dụng thư viện 'kaggle' (có thể cần restart kernel).\n"
     ]
    }
   ],
   "source": [
    "!python setup_kaggle_token.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d719471-7ab3-492d-8d1e-988857307b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kaggle API authentication successful using the copied token file!\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "try:\n",
    "    kaggle.api.authenticate()\n",
    "    print(\"\\nKaggle API authentication successful using the copied token file!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nKaggle API authentication failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e7af007-8c76-41b7-b298-d2263a2f3d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thư mục làm việc hiện tại (notebook): /home/jovyan/work\n",
      "Thư mục gốc dự án (ước tính): /home/jovyan\n",
      "Đường dẫn thư mục Data: /home/jovyan/data\n",
      "Thư mục Data tồn tại: True\n",
      "Thư mục ETL tồn tại: True\n",
      "URI DATABASE:  postgresql+psycopg2://ThangData:password@postgres:5432/olist_dwh\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import zipfile\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "DATASET_ID = 'olistbr/brazilian-ecommerce' # Định danh của dataset trên Kaggle\n",
    "\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Thư mục làm việc hiện tại (notebook): {current_dir}\")\n",
    "\n",
    "BASE_DIR = current_dir.parent\n",
    "print(f\"Thư mục gốc dự án (ước tính): {BASE_DIR}\")\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "ETL_DIR = BASE_DIR / 'etl'\n",
    "\n",
    "print(f\"Đường dẫn thư mục Data: {DATA_DIR}\")\n",
    "\n",
    "print(f\"Thư mục Data tồn tại: {DATA_DIR.exists()}\")\n",
    "print(f\"Thư mục ETL tồn tại: {ETL_DIR.exists()}\")\n",
    "\n",
    "\n",
    "\n",
    "DB_USER = os.getenv('POSTGRES_USER')\n",
    "DB_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "DB_HOST = 'postgres' \n",
    "DB_PORT = '5432'\n",
    "DB_NAME = os.getenv('POSTGRES_DB')\n",
    "DATABASE_URI = f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "print(f\"URI DATABASE:  {DATABASE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2865983-b902-4398-a25f-df2a7191c08e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DOWNLOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e762f120-8030-493a-85b9-8110a650b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 18:03:27,364 - INFO - --- Bắt đầu Script Tải Dữ Liệu Olist ---\n",
      "2025-04-07 18:03:27,365 - INFO - Bắt đầu quá trình xác thực với Kaggle API...\n",
      "2025-04-07 18:03:27,366 - INFO - Xác thực Kaggle API thành công.\n",
      "2025-04-07 18:03:27,366 - INFO - Bắt đầu tải dataset: olistbr/brazilian-ecommerce vào thư mục: /home/jovyan/data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 18:03:28,059 - INFO - Dataset đã được tải về thành công: /home/jovyan/data/brazilian-ecommerce.zip\n",
      "2025-04-07 18:03:28,060 - INFO - Bắt đầu giải nén file: /home/jovyan/data/brazilian-ecommerce.zip\n",
      "2025-04-07 18:03:28,823 - INFO - Giải nén hoàn tất vào thư mục: /home/jovyan/data\n",
      "2025-04-07 18:03:28,835 - INFO - Đã xóa file zip: /home/jovyan/data/brazilian-ecommerce.zip\n",
      "2025-04-07 18:03:28,836 - INFO - --- Script Tải Dữ Liệu Hoàn Thành Thành Công ---\n"
     ]
    }
   ],
   "source": [
    "EXPECTED_FILE_EXAMPLE = DATA_DIR / 'olist_orders_dataset.csv'\n",
    "\n",
    "def check_if_extracted(target_dir, example_file):\n",
    "    return example_file.exists()\n",
    "\n",
    "def download_and_extract_kaggle_dataset(dataset_id, download_path):\n",
    "    \"\"\"\n",
    "    Tải dataset từ Kaggle và giải nén vào thư mục chỉ định.\n",
    "    Sử dụng biến môi trường KAGGLE_USERNAME và KAGGLE_KEY để xác thực.\n",
    "    \"\"\"\n",
    "    # Tạo thư mục download nếu chưa tồn tại\n",
    "    download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Kiểm tra xem dữ liệu đã được giải nén chưa để tránh tải lại\n",
    "    if check_if_extracted(download_path, EXPECTED_FILE_EXAMPLE):\n",
    "        logging.info(f\"Dữ liệu có vẻ đã tồn tại trong thư mục: {download_path}. Bỏ qua tải về.\")\n",
    "        return True\n",
    "\n",
    "    logging.info(\"Bắt đầu quá trình xác thực với Kaggle API...\")\n",
    "    try:\n",
    "        kaggle.api.authenticate()\n",
    "        logging.info(\"Xác thực Kaggle API thành công.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Lỗi xác thực Kaggle API: {e}\")\n",
    "        logging.error(\"Hãy đảm bảo bạn đã cài đặt KAGGLE_USERNAME và KAGGLE_KEY trong file .env hoặc đặt file kaggle.json đúng vị trí.\")\n",
    "        return False\n",
    "\n",
    "    logging.info(f\"Bắt đầu tải dataset: {dataset_id} vào thư mục: {download_path}\")\n",
    "    zip_file_path = None # Khởi tạo để dùng trong finally\n",
    "    try:\n",
    "        # Tải dataset (thường là 1 file zip)\n",
    "        kaggle.api.dataset_download_files(dataset_id, path=download_path, unzip=False) # Tải file zip về trước\n",
    "\n",
    "        # Tìm file zip vừa tải về (tên file thường là tên dataset.zip)\n",
    "        zip_filename = f\"{dataset_id.split('/')[-1]}.zip\"\n",
    "        zip_file_path = download_path / zip_filename\n",
    "\n",
    "        if not zip_file_path.exists():\n",
    "             # Đôi khi tên file zip có thể khác, thử tìm file .zip duy nhất\n",
    "             zip_files = list(download_path.glob('*.zip'))\n",
    "             if len(zip_files) == 1:\n",
    "                  zip_file_path = zip_files[0]\n",
    "             else:\n",
    "                  raise FileNotFoundError(f\"Không tìm thấy file zip dự kiến '{zip_filename}' hoặc file zip duy nhất trong {download_path}\")\n",
    "\n",
    "        logging.info(f\"Dataset đã được tải về thành công: {zip_file_path}\")\n",
    "\n",
    "        logging.info(f\"Bắt đầu giải nén file: {zip_file_path}\")\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_path)\n",
    "        logging.info(f\"Giải nén hoàn tất vào thư mục: {download_path}\")\n",
    "\n",
    "        return True # Trả về True nếu thành công\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Đã xảy ra lỗi trong quá trình tải hoặc giải nén: {e}\")\n",
    "        return False # Trả về False nếu có lỗi\n",
    "\n",
    "    finally:\n",
    "        if zip_file_path and zip_file_path.exists():\n",
    "            try:\n",
    "                os.remove(zip_file_path)\n",
    "                logging.info(f\"Đã xóa file zip: {zip_file_path}\")\n",
    "            except OSError as e:\n",
    "                logging.error(f\"Không thể xóa file zip {zip_file_path}: {e}\")\n",
    "\n",
    "\n",
    "# --- Hàm Main để chạy script ---\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"--- Bắt đầu Script Tải Dữ Liệu Olist ---\")\n",
    "\n",
    "    load_dotenv(dotenv_path=BASE_DIR / '.env')\n",
    "\n",
    "    # Gọi hàm tải và giải nén\n",
    "    success = download_and_extract_kaggle_dataset(DATASET_ID, DATA_DIR)\n",
    "\n",
    "    if success:\n",
    "        logging.info(\"--- Script Tải Dữ Liệu Hoàn Thành Thành Công ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Script Tải Dữ Liệu Gặp Lỗi ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c4ca6-db59-44b0-8a66-42e68da82ea2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "622b745e-b7ee-4f97-a123-b78d95051a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối thành công! Các bảng trong schema 'dwh':\n",
      "- dim_customer\n",
      "- dim_seller\n",
      "- fact_order_delivery\n",
      "- dim_date\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>month_name</th>\n",
       "      <th>month_number</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20160101</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20160102</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>6</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20160103</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>7</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20160104</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20160105</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20160106</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20160107</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>4</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20160108</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>Friday</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20160109</td>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>6</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20160110</td>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>7</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date  day_of_week   day_name  day_of_month  day_of_year  \\\n",
       "0  20160101  2016-01-01            5     Friday             1            1   \n",
       "1  20160102  2016-01-02            6   Saturday             2            2   \n",
       "2  20160103  2016-01-03            7     Sunday             3            3   \n",
       "3  20160104  2016-01-04            1     Monday             4            4   \n",
       "4  20160105  2016-01-05            2    Tuesday             5            5   \n",
       "5  20160106  2016-01-06            3  Wednesday             6            6   \n",
       "6  20160107  2016-01-07            4   Thursday             7            7   \n",
       "7  20160108  2016-01-08            5     Friday             8            8   \n",
       "8  20160109  2016-01-09            6   Saturday             9            9   \n",
       "9  20160110  2016-01-10            7     Sunday            10           10   \n",
       "\n",
       "   week_of_year month_name  month_number  quarter  year  is_weekend  \\\n",
       "0            53    January             1        1  2016       False   \n",
       "1            53    January             1        1  2016        True   \n",
       "2            53    January             1        1  2016        True   \n",
       "3             1    January             1        1  2016       False   \n",
       "4             1    January             1        1  2016       False   \n",
       "5             1    January             1        1  2016       False   \n",
       "6             1    January             1        1  2016       False   \n",
       "7             1    January             1        1  2016       False   \n",
       "8             1    January             1        1  2016        True   \n",
       "9             1    January             1        1  2016        True   \n",
       "\n",
       "   is_weekday  \n",
       "0        True  \n",
       "1       False  \n",
       "2       False  \n",
       "3        True  \n",
       "4        True  \n",
       "5        True  \n",
       "6        True  \n",
       "7        True  \n",
       "8       False  \n",
       "9       False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(DATABASE_URI)\n",
    "    # Thử kết nối và chạy một câu lệnh đơn giản\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'dwh';\"))\n",
    "        print(\"Kết nối thành công! Các bảng trong schema 'dwh':\")\n",
    "        for row in result:\n",
    "            print(f\"- {row[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi kết nối database: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        df_customers = pd.read_sql(\"SELECT * FROM dwh.dim_date LIMIT 10\", connection)\n",
    "        display(df_customers) # display() hiển thị DataFrame đẹp hơn trong Jupyter\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi đọc dữ liệu: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65364d0f-bdc2-4809-9ad7-a8e6627c3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các file CSV và bảng staging tương ứng\n",
    "CSV_FILES = {\n",
    "    'olist_orders_dataset.csv': 'staging.stg_orders',\n",
    "    'olist_order_items_dataset.csv': 'staging.stg_order_items',\n",
    "    'olist_customers_dataset.csv': 'staging.stg_customers',\n",
    "    'olist_sellers_dataset.csv': 'staging.stg_sellers',\n",
    "    'olist_geolocation_dataset.csv': 'staging.stg_geolocation',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc297eae-e9f0-4c73-858e-c08e61349514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_load_to_staging(csv_files_map, data_dir, db_engine):\n",
    "    \"\"\"\n",
    "    Extract dữ liệu từ các file CSV và load vào bảng staging tương ứng.\n",
    "    Xóa dữ liệu cũ trong staging trước khi load.\n",
    "    \"\"\"\n",
    "    logging.info(\"Bắt đầu quá trình Extract và Load vào Staging...\")\n",
    "    with db_engine.connect() as connection:\n",
    "        for csv_file, table_name in csv_files_map.items():\n",
    "            start_time = time.time()\n",
    "            file_path = data_dir / csv_file\n",
    "            if not file_path.exists():\n",
    "                logging.warning(f\"File không tồn tại: {file_path}, bỏ qua.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                logging.info(f\"Đọc file: {csv_file}\")\n",
    "\n",
    "                df = pd.read_csv(file_path, dtype=str)\n",
    "                df['_load_timestamp'] = pd.Timestamp.now() # Thêm metadata thời gian load\n",
    "\n",
    "                logging.info(f\"Load dữ liệu vào bảng: {table_name}\")\n",
    "                # Xóa dữ liệu cũ trong bảng staging\n",
    "                connection.execute(text(f\"TRUNCATE TABLE {table_name};\"))\n",
    "                # Load dữ liệu mới\n",
    "                df.to_sql(\n",
    "                    name=table_name.split('.')[1], # Chỉ lấy tên bảng\n",
    "                    con=connection,\n",
    "                    schema=table_name.split('.')[0], # Chỉ lấy tên schema\n",
    "                    if_exists='append', # Vì đã truncate nên dùng append\n",
    "                    index=False,\n",
    "                    chunksize=10000 # Load theo chunk để tiết kiệm bộ nhớ\n",
    "                )\n",
    "                connection.commit() # Commit sau mỗi bảng staging\n",
    "                end_time = time.time()\n",
    "                logging.info(f\"Hoàn thành load {table_name} trong {end_time - start_time:.2f} giây.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Lỗi khi xử lý file {csv_file} hoặc load vào {table_name}: {e}\")\n",
    "                connection.rollback()\n",
    "\n",
    "    logging.info(\"Hoàn thành Extract và Load vào Staging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e5cb7f8-851c-46b7-86ae-ee562b509e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_load_dimensions(db_engine):\n",
    "    \"\"\"\n",
    "    Transform dữ liệu từ staging và load vào các bảng Dimension\n",
    "    (dim_customer, dim_seller)\n",
    "    \"\"\"\n",
    "    logging.info(\"Bắt đầu quá trình Transform và Load Dimensions (snake_case)...\")\n",
    "    with db_engine.connect() as connection:\n",
    "        # Bắt đầu transaction\n",
    "        with connection.begin(): # Sử dụng transaction cho toàn bộ quá trình load dimension\n",
    "            try:\n",
    "                # --- 1. Chuẩn hóa Geolocation ---\n",
    "                logging.info(\"Chuẩn hóa dữ liệu Geolocation...\")\n",
    "                df_geo = pd.read_sql(\"SELECT * FROM staging.stg_geolocation\", connection)\n",
    "                df_geo['geolocation_city'] = df_geo['geolocation_city'].str.lower().str.strip()\n",
    "                df_geo['geolocation_state'] = df_geo['geolocation_state'].str.upper().str.strip()\n",
    "                # Tạo mapping zip_prefix -> city, state (lấy bản ghi đầu tiên cho mỗi prefix)\n",
    "                # Loại bỏ các prefix trùng lặp, giữ lại bản ghi đầu tiên\n",
    "                geo_map = df_geo.drop_duplicates(subset=['geolocation_zip_code_prefix'], keep='first')\n",
    "                # Tạo index bằng zip_code_prefix để merge dễ dàng\n",
    "                geo_map = geo_map.set_index('geolocation_zip_code_prefix')[['geolocation_city', 'geolocation_state']]\n",
    "                logging.info(f\"Tạo mapping cho {len(geo_map)} zip code prefixes.\")\n",
    "\n",
    "                # --- 2. Load dim_customer ---\n",
    "                logging.info(\"Load dữ liệu vào dwh.dim_customer...\")\n",
    "                start_time = time.time()\n",
    "                df_cust_staging = pd.read_sql(\"SELECT * FROM staging.stg_customers\", connection)\n",
    "\n",
    "                # Merge với geo_map để lấy city/state chuẩn hóa\n",
    "                # Đảm bảo kiểu dữ liệu cột join là string\n",
    "                df_cust_staging['customer_zip_code_prefix'] = df_cust_staging['customer_zip_code_prefix'].astype(str)\n",
    "                df_merged_cust = pd.merge(\n",
    "                    df_cust_staging,\n",
    "                    geo_map,\n",
    "                    left_on='customer_zip_code_prefix',\n",
    "                    right_index=True,\n",
    "                    how='left'\n",
    "                )\n",
    "                \n",
    "                df_dim_cust = df_merged_cust[[\n",
    "                    'customer_id',\n",
    "                    'customer_unique_id',\n",
    "                    'customer_zip_code_prefix',\n",
    "                    'geolocation_city',  \n",
    "                    'geolocation_state'\n",
    "                ]].copy() \n",
    "                \n",
    "                df_dim_cust = df_dim_cust.rename(columns={\n",
    "                    'geolocation_city': 'customer_city',\n",
    "                    'geolocation_state': 'customer_state'\n",
    "                })\n",
    "\n",
    "                # Xử lý NULL sau merge và chuẩn hóa thêm nếu cần\n",
    "                df_dim_cust['customer_city'] = df_dim_cust['customer_city'].fillna('Unknown')\n",
    "                df_dim_cust['customer_state'] = df_dim_cust['customer_state'].fillna('NA')\n",
    "                # Optional: Thêm state_name, region dựa trên state (cần có mapping riêng)\n",
    "                # df_dim_cust['customer_state_name'] = df_dim_cust['customer_state'].map(state_mapping_dict)\n",
    "                # df_dim_cust['customer_region'] = df_dim_cust['customer_state'].map(region_mapping_dict)\n",
    "\n",
    "                dim_customer_cols = [\n",
    "                    'customer_id', 'customer_unique_id', 'customer_zip_code_prefix',\n",
    "                    'customer_city', 'customer_state' #, 'customer_state_name', 'customer_region'\n",
    "                ]\n",
    "                df_dim_cust = df_dim_cust[dim_customer_cols]\n",
    "\n",
    "                # Xử lý SCD Type 2 (Phiên bản đơn giản - Chỉ load bản ghi mới nhất/duy nhất)\n",
    "                # Lấy bản ghi cuối cùng cho mỗi customer_id nếu có trùng lặp trong staging\n",
    "                df_dim_cust = df_dim_cust.drop_duplicates(subset=['customer_id'], keep='last')\n",
    "\n",
    "                # Thêm các cột SCD (snake_case)\n",
    "                df_dim_cust['effective_start_date'] = pd.Timestamp.now()\n",
    "                df_dim_cust['effective_end_date'] = pd.NaT # NULL trong DB\n",
    "                df_dim_cust['is_current'] = True\n",
    "\n",
    "                # Xóa dữ liệu cũ trong DimCustomer (cho lần load đầu hoặc full load)\n",
    "                logging.info(\"Truncating dwh.dim_customer...\")\n",
    "                connection.execute(text(\"TRUNCATE TABLE dwh.dim_customer CASCADE;\")) # CASCADE để xóa FK refs\n",
    "                # Load vào dwh.dim_customer\n",
    "                logging.info(f\"Loading {len(df_dim_cust)} rows into dwh.dim_customer...\")\n",
    "                df_dim_cust.to_sql(\n",
    "                    name='dim_customer', \n",
    "                    con=connection,\n",
    "                    schema='dwh',\n",
    "                    if_exists='append', # Đã truncate nên dùng append\n",
    "                    index=False,\n",
    "                    chunksize=10000\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                logging.info(f\"Hoàn thành load dim_customer trong {end_time - start_time:.2f} giây.\")\n",
    "\n",
    "                \n",
    "                # ------------ 3. LOAD DIM_SELLER ------------------\n",
    "                logging.info(\"Load dữ liệu vào dwh.dim_seller...\")\n",
    "                start_time = time.time()\n",
    "                df_seller_staging = pd.read_sql(\"SELECT * FROM staging.stg_sellers\", connection)\n",
    "\n",
    "                # Merge với geo_map\n",
    "                df_seller_staging['seller_zip_code_prefix'] = df_seller_staging['seller_zip_code_prefix'].astype(str)\n",
    "                df_merged_seller = pd.merge(\n",
    "                    df_seller_staging,\n",
    "                    geo_map,\n",
    "                    left_on='seller_zip_code_prefix',\n",
    "                    right_index=True,\n",
    "                    how='left'\n",
    "                )\n",
    "                \n",
    "                df_dim_seller = df_merged_seller[[\n",
    "                    'seller_id',\n",
    "                    'seller_zip_code_prefix',\n",
    "                    'geolocation_city', \n",
    "                    'geolocation_state' \n",
    "                ]].copy() \n",
    "                \n",
    "                df_dim_seller = df_dim_seller.rename(columns={\n",
    "                    'geolocation_city': 'seller_city',\n",
    "                    'geolocation_state': 'seller_state'\n",
    "                })\n",
    "                df_dim_seller['seller_city'] = df_dim_seller['seller_city'].fillna('Unknown')\n",
    "                df_dim_seller['seller_state'] = df_dim_seller['seller_state'].fillna('NA')\n",
    "                # Optional: Thêm state_name, region\n",
    "\n",
    "                # Chọn cột (snake_case)\n",
    "                dim_seller_cols = [\n",
    "                    'seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state'\n",
    "                    # , 'seller_state_name', 'seller_region'\n",
    "                ]\n",
    "                df_dim_seller = df_dim_seller[dim_seller_cols]\n",
    "\n",
    "                # Xử lý SCD Type 2 (Đơn giản)\n",
    "                df_dim_seller = df_dim_seller.drop_duplicates(subset=['seller_id'], keep='last')\n",
    "                df_dim_seller['effective_start_date'] = pd.Timestamp.now()\n",
    "                df_dim_seller['effective_end_date'] = pd.NaT\n",
    "                df_dim_seller['is_current'] = True\n",
    "\n",
    "                # Xóa dữ liệu cũ (cho lần load đầu)\n",
    "                logging.info(\"Truncating dwh.dim_seller...\")\n",
    "                connection.execute(text(\"TRUNCATE TABLE dwh.dim_seller CASCADE;\"))\n",
    "                # Load vào dwh.dim_seller\n",
    "                logging.info(f\"Loading {len(df_dim_seller)} rows into dwh.dim_seller...\")\n",
    "                df_dim_seller.to_sql(\n",
    "                    name='dim_seller', # Tên bảng snake_case\n",
    "                    con=connection,\n",
    "                    schema='dwh',\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    chunksize=1000\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                logging.info(f\"Hoàn thành load dim_seller trong {end_time - start_time:.2f} giây.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Lỗi trong quá trình Transform và Load Dimensions: {e}\")\n",
    "                # Transaction sẽ tự rollback khi thoát khỏi 'with connection.begin()' nếu có lỗi\n",
    "                raise e # Ném lại lỗi để dừng ETL\n",
    "\n",
    "    logging.info(\"Hoàn thành Transform và Load Dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3be33928-8e85-40f0-84ef-3d5853fdaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_load_fact(db_engine):\n",
    "    \"\"\"\n",
    "    Transform dữ liệu từ staging, lookup keys từ Dimensions,\n",
    "    và load vào fact_order_delivery\n",
    "    \"\"\"\n",
    "    logging.info(\"Bắt đầu quá trình Transform và Load Fact Table...\")\n",
    "    with db_engine.connect() as connection:\n",
    "        with connection.begin(): # Sử dụng transaction\n",
    "            try:\n",
    "                # --- 1. Đọc dữ liệu cần thiết ---\n",
    "                logging.info(\"Đọc dữ liệu từ staging và dimensions...\")\n",
    "                df_orders = pd.read_sql(\"SELECT * FROM staging.stg_orders\", connection)\n",
    "                df_items = pd.read_sql(\"SELECT * FROM staging.stg_order_items\", connection)\n",
    "                df_dim_date = pd.read_sql('SELECT date_key, full_date FROM dwh.dim_date', connection, parse_dates=['full_date'])\n",
    "                df_dim_cust = pd.read_sql('SELECT customer_key, customer_id FROM dwh.dim_customer WHERE is_current = TRUE', connection)\n",
    "                df_dim_seller = pd.read_sql('SELECT seller_key, seller_id FROM dwh.dim_seller WHERE is_current = TRUE', connection)\n",
    "\n",
    "\n",
    "                # --- 2. Xử lý và Tổng hợp Order Items ---\n",
    "                logging.info(\"Tổng hợp dữ liệu Order Items...\")\n",
    "                df_items['price'] = pd.to_numeric(df_items['price'], errors='coerce').fillna(0)\n",
    "                df_items['freight_value'] = pd.to_numeric(df_items['freight_value'], errors='coerce').fillna(0)\n",
    "                df_items_agg = df_items.groupby('order_id').agg(\n",
    "                    item_count=('order_item_id', 'count'),\n",
    "                    total_freight_value=('freight_value', 'sum'),\n",
    "                    total_price=('price', 'sum'),\n",
    "                    seller_id=('seller_id', 'first')\n",
    "                ).reset_index()\n",
    "\n",
    "                # --- 3. Kết hợp Orders và Items Aggregated ---\n",
    "                logging.info(\"Kết hợp Orders và Items Aggregated...\")\n",
    "                df_fact = pd.merge(df_orders, df_items_agg, on='order_id', how='inner')\n",
    "\n",
    "                # --- 4. Chuyển đổi kiểu dữ liệu Ngày tháng trong Orders ---\n",
    "                logging.info(\"Chuyển đổi kiểu dữ liệu ngày tháng...\")\n",
    "                date_cols_ts = [\n",
    "                    'order_purchase_timestamp', 'order_approved_at',\n",
    "                    'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "                    'order_estimated_delivery_date'\n",
    "                ]\n",
    "                for col in date_cols_ts:\n",
    "                    df_fact[col] = pd.to_datetime(df_fact[col], errors='coerce')\n",
    "\n",
    "                date_cols_date = {\n",
    "                    'order_purchase_timestamp': 'purchase_date',\n",
    "                    'order_approved_at': 'approved_date',\n",
    "                    'order_delivered_carrier_date': 'delivered_carrier_date',\n",
    "                    'order_delivered_customer_date': 'delivered_customer_date',\n",
    "                    'order_estimated_delivery_date': 'estimated_delivery_date'\n",
    "                }\n",
    "                for ts_col, date_col in date_cols_date.items():\n",
    "                     df_fact[date_col] = df_fact[ts_col].dt.date\n",
    "\n",
    "                # --- 5. Tính toán các Measures ---\n",
    "                logging.info(\"Tính toán các Measures...\")\n",
    "                df_fact['delivery_time_days'] = (pd.to_datetime(df_fact['delivered_customer_date'], errors='coerce') - pd.to_datetime(df_fact['approved_date'], errors='coerce')).dt.days\n",
    "                df_fact['estimated_delivery_time_days'] = (pd.to_datetime(df_fact['estimated_delivery_date'], errors='coerce') - pd.to_datetime(df_fact['approved_date'], errors='coerce')).dt.days\n",
    "                df_fact['delivery_time_difference_days'] = (pd.to_datetime(df_fact['delivered_customer_date'], errors='coerce') - pd.to_datetime(df_fact['estimated_delivery_date'], errors='coerce')).dt.days\n",
    "                df_fact['time_to_approve_hours'] = (df_fact['order_approved_at'] - df_fact['order_purchase_timestamp']) / pd.Timedelta(hours=1)\n",
    "                df_fact['seller_processing_hours'] = (df_fact['order_delivered_carrier_date'] - df_fact['order_approved_at']) / pd.Timedelta(hours=1)\n",
    "                df_fact['carrier_shipping_hours'] = (df_fact['order_delivered_customer_date'] - df_fact['order_delivered_carrier_date']) / pd.Timedelta(hours=1)\n",
    "                hour_cols = ['time_to_approve_hours', 'seller_processing_hours', 'carrier_shipping_hours']\n",
    "                for col in hour_cols:\n",
    "                    df_fact[col] = df_fact[col].round(2)\n",
    "                df_fact['is_late_delivery_flag'] = (df_fact['delivery_time_difference_days'] > 0) & (df_fact['delivered_customer_date'].notna())\n",
    "                df_fact['is_late_delivery_flag'] = df_fact['is_late_delivery_flag'].fillna(False).astype(bool)\n",
    "\n",
    "                # ----  CHECK các giá trị ÂM -------\n",
    "                logging.info(\"Setting negative time measures to None...\")\n",
    "                time_measure_cols = [\n",
    "                    'delivery_time_days', 'estimated_delivery_time_days', 'delivery_time_difference_days',\n",
    "                    'time_to_approve_hours', 'seller_processing_hours', 'carrier_shipping_hours'\n",
    "                ]\n",
    "                for col in time_measure_cols:\n",
    "                    if col in df_fact.columns:\n",
    "                        df_fact.loc[df_fact[col] < 0, col] = None\n",
    "                    else:\n",
    "                         logging.warning(f\"Column {col} not found for negative check.\")\n",
    "\n",
    "                # --- 6. Lookup Dimension Keys ---\n",
    "                logging.info(\"Lookup Dimension Keys...\")\n",
    "                date_lookup_cols = {\n",
    "                    'purchase_date': 'purchase_date_key',\n",
    "                    'approved_date': 'approved_date_key',\n",
    "                    'delivered_carrier_date': 'delivered_carrier_date_key',\n",
    "                    'delivered_customer_date': 'delivered_customer_date_key',\n",
    "                    'estimated_delivery_date': 'estimated_delivery_date_key'\n",
    "                }\n",
    "                # Chuyển cột date trong fact sang datetime để join\n",
    "                for date_col_fact in date_lookup_cols.keys():\n",
    "                     df_fact[date_col_fact] = pd.to_datetime(df_fact[date_col_fact], errors='coerce')\n",
    "\n",
    "                # Join với DimDate cho từng cột ngày\n",
    "                for date_col_fact, date_key_col in date_lookup_cols.items():\n",
    "                    temp_dim_date = df_dim_date.rename(columns={'date_key': date_key_col})\n",
    "                    df_fact = pd.merge(\n",
    "                        df_fact,\n",
    "                        temp_dim_date[['full_date', date_key_col]],\n",
    "                        left_on=date_col_fact,\n",
    "                        right_on='full_date',\n",
    "                        how='left'\n",
    "                    )\n",
    "                    df_fact = df_fact.drop(columns=['full_date']) # Bỏ cột full_date sau mỗi lần merge\n",
    "\n",
    "                # Join với DimCustomer\n",
    "                df_fact = pd.merge(\n",
    "                    df_fact,\n",
    "                    df_dim_cust[['customer_key', 'customer_id']],\n",
    "                    on='customer_id',\n",
    "                    how='left'\n",
    "                )\n",
    "\n",
    "                # Join với DimSeller\n",
    "                df_fact = pd.merge(\n",
    "                    df_fact,\n",
    "                    df_dim_seller[['seller_key', 'seller_id']],\n",
    "                    on='seller_id',\n",
    "                    how='left'\n",
    "                )\n",
    "\n",
    "                logging.info(\"Handling failed lookups and preparing key data types...\")\n",
    "                date_key_cols_list = list(date_lookup_cols.values())\n",
    "                dim_key_cols_list = ['customer_key', 'seller_key']\n",
    "                all_key_cols = date_key_cols_list + dim_key_cols_list\n",
    "\n",
    "                for col in all_key_cols:\n",
    "                    if col not in df_fact.columns:\n",
    "                        logging.warning(f\"Key column '{col}' missing after merges. Adding as pd.NA.\")\n",
    "                        df_fact[col] = pd.NA # Sử dụng pd.NA là tốt nhất cho nullable int\n",
    "                    else:\n",
    "                        # QUAN TRỌNG: KHÔNG fillna(-1) một cách mù quáng nữa.\n",
    "                        # Chỉ fillna(-1) cho customer/seller keys NẾU bạn đã tạo dòng Unknown=-1 trong Dim.\n",
    "                        # Nếu không, hãy để NaN/NA để nó thành NULL trong DB.\n",
    "                        if col in dim_key_cols_list:\n",
    "                            # Tạm thời vẫn fill -1 cho dimension keys nếu bạn muốn (cần có dòng -1 trong dim)\n",
    "                            # Hoặc comment dòng fillna này để nó thành NULL\n",
    "                            df_fact[col] = df_fact[col].fillna(-1)\n",
    "                            pass # Giữ logic cũ cho dim keys (cẩn thận nếu không có dòng -1)\n",
    "                        # else: # col là date_key_col\n",
    "                             # KHÔNG LÀM GÌ CẢ với fillna cho date keys, để NaN/NA tự nhiên\n",
    "\n",
    "                    # Chuyển đổi sang kiểu số nullable để to_sql xử lý NaN/NA thành NULL\n",
    "                    # Sử dụng float trước để xử lý các kiểu dữ liệu không đồng nhất có thể có\n",
    "                    df_fact[col] = pd.to_numeric(df_fact[col], errors='coerce')\n",
    "                    # Chuyển sang Int64 của Pandas để biểu diễn integer nullable\n",
    "                    # Điều này giúp to_sql hiểu rõ hơn ý định gửi NULL\n",
    "                    df_fact[col] = df_fact[col].astype('Int64') # 'Int64' (chữ I viết hoa) là nullable integer type\n",
    "\n",
    "                # --- 7. Chuẩn bị dữ liệu cuối cùng cho Fact ---\n",
    "                logging.info(\"Chuẩn bị dữ liệu cuối cùng cho fact_order_delivery...\")\n",
    "                df_fact = df_fact.rename(columns={'order_id': 'order_id', 'order_status': 'order_status'})\n",
    "                df_fact['order_count'] = 1\n",
    "                df_fact['dw_load_timestamp'] = pd.Timestamp.now()\n",
    "                final_fact_columns = [\n",
    "                    'order_id', 'purchase_date_key', 'approved_date_key', 'delivered_carrier_date_key',\n",
    "                    'delivered_customer_date_key', 'estimated_delivery_date_key', 'customer_key', 'seller_key',\n",
    "                    'order_status', 'delivery_time_days', 'estimated_delivery_time_days', 'delivery_time_difference_days',\n",
    "                    'is_late_delivery_flag', 'time_to_approve_hours', 'seller_processing_hours', 'carrier_shipping_hours',\n",
    "                    'item_count', 'total_freight_value', 'total_price', 'order_count', 'dw_load_timestamp'\n",
    "                ]\n",
    "                missing_cols = [col for col in final_fact_columns if col not in df_fact.columns]\n",
    "                if missing_cols:\n",
    "                    logging.error(f\"Thiếu các cột trong Fact DataFrame: {missing_cols}\")\n",
    "                    raise ValueError(f\"Missing columns required for fact table: {missing_cols}\")\n",
    "                df_fact_final = df_fact[final_fact_columns]\n",
    "\n",
    "\n",
    "                # --- 8. Load dữ liệu vào Fact Table ---\n",
    "                logging.info(f\"Load {len(df_fact_final)} dòng vào dwh.fact_order_delivery...\")\n",
    "                start_time = time.time()\n",
    "                logging.info(\"Truncating dwh.fact_order_delivery...\")\n",
    "                connection.execute(text(\"TRUNCATE TABLE dwh.fact_order_delivery;\"))\n",
    "                df_fact_final.to_sql(\n",
    "                    name='fact_order_delivery',\n",
    "                    con=connection,\n",
    "                    schema='dwh',\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    chunksize=10000,\n",
    "                    # method='multi' # Có thể thử method='multi' nếu mặc định chậm\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                logging.info(f\"Hoàn thành load fact_order_delivery trong {end_time - start_time:.2f} giây.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Lỗi trong quá trình Transform và Load Fact Table: {e}\")\n",
    "                raise e\n",
    "\n",
    "    logging.info(\"Hoàn thành Transform và Load Fact Table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49ea851a-4cf6-449c-9782-a3f18ea295e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 18:37:15,645 - INFO - === BẮT ĐẦU QUÁ TRÌNH ETL ===\n",
      "2025-04-07 18:37:15,646 - INFO - Bắt đầu quá trình Extract và Load vào Staging...\n",
      "2025-04-07 18:37:15,647 - INFO - Đọc file: olist_orders_dataset.csv\n",
      "2025-04-07 18:37:15,895 - INFO - Load dữ liệu vào bảng: staging.stg_orders\n",
      "2025-04-07 18:37:20,903 - INFO - Hoàn thành load staging.stg_orders trong 5.26 giây.\n",
      "2025-04-07 18:37:20,903 - INFO - Đọc file: olist_order_items_dataset.csv\n",
      "2025-04-07 18:37:21,065 - INFO - Load dữ liệu vào bảng: staging.stg_order_items\n",
      "2025-04-07 18:37:26,548 - INFO - Hoàn thành load staging.stg_order_items trong 5.64 giây.\n",
      "2025-04-07 18:37:26,549 - INFO - Đọc file: olist_customers_dataset.csv\n",
      "2025-04-07 18:37:26,672 - INFO - Load dữ liệu vào bảng: staging.stg_customers\n",
      "2025-04-07 18:37:30,233 - INFO - Hoàn thành load staging.stg_customers trong 3.68 giây.\n",
      "2025-04-07 18:37:30,234 - INFO - Đọc file: olist_sellers_dataset.csv\n",
      "2025-04-07 18:37:30,245 - INFO - Load dữ liệu vào bảng: staging.stg_sellers\n",
      "2025-04-07 18:37:30,333 - INFO - Hoàn thành load staging.stg_sellers trong 0.10 giây.\n",
      "2025-04-07 18:37:30,334 - INFO - Đọc file: olist_geolocation_dataset.csv\n",
      "2025-04-07 18:37:31,300 - INFO - Load dữ liệu vào bảng: staging.stg_geolocation\n",
      "2025-04-07 18:38:00,620 - INFO - Hoàn thành load staging.stg_geolocation trong 30.29 giây.\n",
      "2025-04-07 18:38:00,621 - INFO - Hoàn thành Extract và Load vào Staging.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"=== BẮT ĐẦU QUÁ TRÌNH ETL ===\")\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    extract_load_to_staging(CSV_FILES, DATA_DIR, engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8561c6d5-86b3-4d92-bc3b-5f864c0602f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 18:38:05,417 - INFO - Bắt đầu quá trình Transform và Load Dimensions (snake_case)...\n",
      "2025-04-07 18:38:05,418 - INFO - Chuẩn hóa dữ liệu Geolocation...\n",
      "2025-04-07 18:38:09,421 - INFO - Tạo mapping cho 19015 zip code prefixes.\n",
      "2025-04-07 18:38:09,421 - INFO - Load dữ liệu vào dwh.dim_customer...\n",
      "2025-04-07 18:38:09,960 - INFO - Truncating dwh.dim_customer...\n",
      "2025-04-07 18:38:09,975 - INFO - Loading 99441 rows into dwh.dim_customer...\n",
      "2025-04-07 18:38:16,983 - INFO - Hoàn thành load dim_customer trong 7.56 giây.\n",
      "2025-04-07 18:38:16,984 - INFO - Load dữ liệu vào dwh.dim_seller...\n",
      "2025-04-07 18:38:17,008 - INFO - Truncating dwh.dim_seller...\n",
      "2025-04-07 18:38:17,032 - INFO - Loading 3095 rows into dwh.dim_seller...\n",
      "2025-04-07 18:38:17,171 - INFO - Hoàn thành load dim_seller trong 0.19 giây.\n",
      "2025-04-07 18:38:17,176 - INFO - Hoàn thành Transform và Load Dimensions.\n"
     ]
    }
   ],
   "source": [
    "transform_and_load_dimensions(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e93ac65-f65c-43ba-86f0-46ecca71cc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 19:03:28,771 - INFO - Bắt đầu quá trình Transform và Load Fact Table...\n",
      "2025-04-07 19:03:28,772 - INFO - Đọc dữ liệu từ staging và dimensions...\n",
      "2025-04-07 19:03:30,054 - INFO - Tổng hợp dữ liệu Order Items...\n",
      "2025-04-07 19:03:30,198 - INFO - Kết hợp Orders và Items Aggregated...\n",
      "2025-04-07 19:03:30,286 - INFO - Chuyển đổi kiểu dữ liệu ngày tháng...\n",
      "2025-04-07 19:03:30,456 - INFO - Tính toán các Measures...\n",
      "2025-04-07 19:03:30,515 - INFO - Setting negative time measures to None...\n",
      "2025-04-07 19:03:30,518 - INFO - Lookup Dimension Keys...\n",
      "2025-04-07 19:03:30,798 - INFO - Handling failed lookups and preparing key data types...\n",
      "2025-04-07 19:03:30,823 - INFO - Chuẩn bị dữ liệu cuối cùng cho fact_order_delivery...\n",
      "2025-04-07 19:03:30,838 - INFO - Load 98666 dòng vào dwh.fact_order_delivery...\n",
      "2025-04-07 19:03:30,839 - INFO - Truncating dwh.fact_order_delivery...\n",
      "2025-04-07 19:03:43,824 - INFO - Hoàn thành load fact_order_delivery trong 12.98 giây.\n",
      "2025-04-07 19:03:43,837 - INFO - Hoàn thành Transform và Load Fact Table.\n"
     ]
    }
   ],
   "source": [
    "transform_and_load_fact(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d95f3-efd1-483a-aaec-525c8c1ffff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "logging.info(f\"=== KẾT THÚC QUÁ TRÌNH ETL TRONG {total_end_time - total_start_time:.2f} GIÂY ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dce957-e359-4128-989b-b8d410e0793d",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "045434f2-e5c0-49c0-b1ae-2fafcc6e824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.6, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: /home/jovyan/work\n",
      "plugins: anyio-4.0.0\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_unit_etl.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.44s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_unit_etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a9130a4-7d55-4f0f-8afd-c5f1fa51517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.6, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: /home/jovyan/work\n",
      "plugins: anyio-4.0.0\n",
      "collected 0 items                                                              \u001b[0m\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/conftest.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
